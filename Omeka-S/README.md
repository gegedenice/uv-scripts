# OWNER/REPO_NAME – Omeka S Multimodal Image Collection with UForm + USearch Index

This dataset exposes an **image collection from an Omeka S instance** together with:

- the **original descriptive metadata** (as a Hugging Face `datasets` table),
- **pre-computed UForm embeddings** for images and titles,
- a **USearch vector index** for fast multimodal search.

It is intended as a **ready-to-use playground** for:

- multimodal semantic search (text → image, image → image),
- experimenting with small / efficient embedding models (UForm),
- testing vector search libraries (USearch) on real cultural heritage data.

---

## Data source

- **Extraction method:** 
  - Omeka S REST API, using [`omeka-s-api-client`](https://github.com/gegedenice/omeka-s-api-client) from an Omeka S instance root url (for example `https://humazur.univ-cotedazur.fr`)
  - Given a collection id (item-set id)
  - Given metadata fields to harvest (for example `dcterms:title,dcterms:description,dcterms:spatial,bibo:identifier`)

- Each row corresponds to **one Omeka S item** that has at least one image media.

> ⚠️ Images themselves are **not mirrored** in this dataset.  
> Instead, we store **remote URLs** (`images_urls`) pointing to the original Omeka S server.

---

## Files in this dataset

### 1. Metadata table (Hugging Face `datasets`)

The core dataset is a standard HF `Dataset` with columns like:

- `id` – simple running index (1..N)
- `item_id` – Omeka S internal item identifier
- `Identifier` – value of `dcterms:identifier` (if present)
- `Title` – value of `dcterms:title` (mandatory in Omeka S)
- `images_urls` – list of image URLs (`o:original_url` from Omeka S media)
- other columns – depending on which Omeka properties were requested  
  (e.g. `Description`, `Creator`, `Date`, `Spatial`, `Subject`, …)

These columns come from `OmekaSClient.digest_item_data` using either:

- default prefixes `OmekaSClient._DEFAULT_PARSE_METADATA`, or  
- a custom list provided when generating the dataset.

### 2. Precomputed embeddings & index

All binary files below are generated by the companion `uv` script:

- **`image_embeddings.fbin`**  
  - Binary float32 matrix of shape `(N, D)`  
  - `N` = number of items with at least one image  
  - `D` = embedding dimension (`NDIM`, default `256`)  
  - Encoded with **UForm image encoder** (`unum-cloud/uform3-image-text-multilingual-base`), truncated to the first `D` dimensions.

- **`text_embeddings.fbin`**  
  - Binary float32 matrix of shape `(N, D)`  
  - Texts are **titles only** (mandatory `dcterms:title` field), formatted as:
    ```text
    Titre: <Title>
    ```
  - Encoded with **UForm text encoder** (`unum-cloud/uform3-image-text-multilingual-base`), truncated to the first `D` dimensions.

- **`labels.npy`**  
  - NumPy array of shape `(N,)`, dtype `int64`  
  - Contains the **Omeka `item_id`** for each row, aligned with the rows of the embedding matrices.

- **`usearch_index.usearch`**  
  - A **USearch** index built on **fused image+text embeddings**.  
  - For each item:
    ```python
    img_vec = image_embedding  # normalized
    txt_vec = text_embedding   # normalized
    fused_vec = 0.5 * img_vec + 0.5 * txt_vec
    fused_vec = fused_vec / ||fused_vec||
    index.add(label=item_id, vector=fused_vec)
    ```
  - Metric: **cosine** (`metric="cos"`)
  - dtype: `f32`
  - Each index entry is keyed by the **Omeka `item_id`**.

---

## Embedding model details

- **Model:** `unum-cloud/uform3-image-text-multilingual-base`
- **Type:** multimodal image–text encoder, multilingual
- **Back-end:** ONNX Runtime via `uform`
- **Embedding dimension:** full model dimension ≥ 256; we keep only the **first `D=256` components** (`NDIM` in the script).

The goal is to provide a **compact latent space** suitable for:

- lightweight multimodal search,
- on-premises / smaller machines,
- experiments with USearch and tiny models.

---

## How to use

### 0. Install dependencies

```bash
uv pip install datasets huggingface_hub usearch uform onnxruntime pillow numpy requests
```
### 1. Load USearch index and the metadata table

```
from huggingface_hub import hf_hub_download

repo_id = "OWNER/REPO_NAME"
index_path = hf_hub_download(repo_id, "usearch_index.usearch", repo_type="dataset")
labels_path = hf_hub_download(repo_id, "labels.npy", repo_type="dataset")

index = Index.restore(index_path)
labels = np.load(labels_path)

print(index.size)   # number of entries
print(index.ndim)   # dimension (should be 256)
```

```
from datasets import load_dataset

ds = load_dataset("OWNER/REPO_NAME")
print(ds)

# Access rows
row = ds["train"][0]
print(row["item_id"], row["Title"], row["images_urls"])
```

### 2. Text -> Image search

```
from uform import get_model, Modality
import numpy as np

# Load UForm model and processors
processors, models = get_model("unum-cloud/uform3-image-text-multilingual-base", device="cpu")
processor_text = processors[Modality.TEXT_ENCODER]
model_text = models[Modality.TEXT_ENCODER]

def get_text_embedding(text: str) -> np.ndarray:
    text_data = processor_text(text)
    embedding = model_text.encode(text_data, return_features=False)
    # Ensure the embedding is a 1D array for stacking later
    return embedding.flatten().astype("float32")

def search_text(query: str, k=3) -> np.ndarray:
    vector = get_text_embedding(query)
    matches = index.search(vector.flatten(), k)
    return matches.keys, matches.distances

# Perform a search
keys, distance = search_text("chateau")

id_to_idx = {int(row["item_id"]): i for i, row in enumerate(ds["train"])}
for label in keys:
    row_idx = id_to_idx.get(int(label))
    if row_idx is not None:
        row = ds["train"][row_idx]
        print(label, row["Title"], row["images_urls"][0])
```

### 3. Image -> Image search

```
from io import BytesIO
import requests
from PIL import Image
from uform import get_model, Modality
import numpy as np

# Load UForm model and processors
processors, models = get_model("unum-cloud/uform3-image-text-multilingual-base", device="cpu")
processor_image = processors[Modality.IMAGE_ENCODER]
model_image = models[Modality.IMAGE_ENCODER]

def get_image_embedding(pil_img: Image.Image) -> np.ndarray:
    image_data = processor_image(pil_img:)
    embedding = model_image.encode(image_data, return_features=False)
    # Ensure the embedding is a 1D array for stacking later
    return embedding.flatten().astype("float32")

def search_image(pil_image: str, k=3) -> np.ndarray:
    vector = get_image_embedding(Image(pil_image))
    matches = index.search(vector.flatten(), k)
    return matches.keys, matches.distances

def search_image_from_url(url: str, k: int = 10):
    resp = requests.get(url, stream=True)
    resp.raise_for_status()
    pil_img = Image.open(BytesIO(resp.content)).convert("RGB")
    vector = get_image_embedding(Image(pil_image))
    matches = index.search(vector.flatten(), k)
    return matches.keys, matches.distances
```